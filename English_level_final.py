#!/usr/bin/env python
# coding: utf-8

# <h1>Table of Contents<span class="tocSkip"></span></h1>
# <div class="toc"><ul class="toc-item"></ul></div>

# In[7]:


import streamlit as st
import pandas as pd
import pickle
from pickle import dump, load
import os
import re
import pysrt
import zipfile
import requests
import spacy
import string
import en_core_web_sm

from PIL import Image
from zipfile import ZipFile
from datetime import datetime
from requests import Session, RequestException
from typing import Optional, Union

from urllib.parse import urljoin
from bs4 import BeautifulSoup, Tag, NavigableString


MAIN_URL = 'https://www.opensubtitles.org/'

def find_tag(
        session: BeautifulSoup,
        tag: str,
        attrs: dict[str]
    ) -> Optional[Union[Tag, NavigableString]]:
    result = session.find(name=tag, attrs=attrs)
    if result is None:
        raise Exception(
        st.sidebar.warning(f'–¢–µ–≥ {tag}, —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ {attrs} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.', icon="‚ö†Ô∏è")
        )
    return result


# In[8]:


def parse_srt_url(
        title: str, year: int,
        season: Optional[int] = None,
        episode: Optional[int] = None,
        is_series: bool = False,) -> tuple[str]:
    movie_name = '+'.join(title.split())
    link = (f"/en/search2/sublanguageid-eng/searchonlymovies-on/"
            f"movieyearsign-1/movieyear-{year}/"
            f"moviename-{movie_name}")
    if is_series:
        link = (f'/en/search2/sublanguageid-eng/searchonlytvseries-on/'
                f'season-{season}/episode-{episode}/'
                f'movieyearsign-1/'
                f'movieyear-{year}/moviename-{movie_name}')
    link = urljoin(MAIN_URL, link)
    session = Session()
    try:
        response = session.get(link)
        response.encoding = 'utf-8'
    except RequestException:
        raise RequestException(
            st.sidebar.warning(f'–í–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {link}. –£—Ç–æ—á–Ω–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤–≤–µ–¥–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.', icon="‚ö†Ô∏è")
            )
    soup = BeautifulSoup(response.text, features='lxml')
    if 'subtitle-language-selector-link' not in soup:
        movie_link = find_tag(soup, 'a', {'class': 'bnone'})['href']
        response = session.get(urljoin(MAIN_URL, movie_link))
        soup = BeautifulSoup(response.text, 'lxml')
    movie_name = soup.find(
            'a', {'class': 'bt-dwl external adds_trigger'}
        )
    if movie_name is not None:
        movie_name = movie_name['data-product-title']
    else:
        movie_name = find_tag(soup, 'a', attrs={'class': 'bnone'}).text
    searched_tag = find_tag(
        soup, 'a', {'href': re.compile(r'/en/subtitleserve/sub/\d+$')}
    )
    poster_link = soup.find('img', attrs={'alt': 'film'})
    poster_link = urljoin(MAIN_URL, poster_link['src'])
    str_link = urljoin(MAIN_URL, searched_tag['href'])
    return movie_name.replace("\n", ""), poster_link, str_link



st.title('–£–∑–Ω–∞–π—Ç–µ —É—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ñ–∏–ª—å–º–∞(—Å–µ—Ä–∏–∞–ª–∞) üé•')

st.subheader('–í–≤–µ–¥–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –¥–∞–Ω–Ω—ã–µ –æ —Ñ–∏–ª—å–º–µ(—Å–µ—Ä–∏–∞–ª–µ)')
warning = '–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –≤–≤–µ–¥–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö!'

type_film = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–ø ", options=("–§–∏–ª—å–º", "–°–µ—Ä–∏–∞–ª"), key='type_film')
if type_film == '–§–∏–ª—å–º':
    title = st.text_input("–ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∏–ª—å–º–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ", key='title') 
    year = st.number_input('–ì–æ–¥ —Ä–µ–ª–∏–∑–∞', key='year')
    if 1920 > year < datetime.now().year:
        st.info(warning, icon="‚ö†Ô∏è")
    season = 0
    episode = 0
else:   
    title = st.text_input("–í–≤–µ–¥–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–µ—Ä–∏–∞–ª–∞ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ", key='title') 
    year = st.number_input('–ì–æ–¥ —Ä–µ–ª–∏–∑–∞', key='year')
    if 1920 > year < datetime.now().year:
        st.info(warning, icon="‚ö†Ô∏è")    
    season = st.number_input('–°–µ–∑–æ–Ω', key='season')
    episode = st.number_input('–°–µ—Ä–∏—è', key='episode')


year = int(year)
season = int(season)
episode = int(episode)


is_series = False
if type_film == "–°–µ—Ä–∏–∞–ª":
    is_series = True
    
if st.button("–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —É—Ä–æ–≤–µ–Ω—å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"):
    name, path_img, path = parse_srt_url(title=title, year=year, season=season, episode=episode, is_series=is_series)


    HTML = r'<.*?>' # html —Ç—ç–≥–∏ –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
    TAG = r'{.*?}' # —Ç—ç–≥–∏ –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
    COMMENTS = r'[\(\[][A-Za-z ]+[\)\]]' # –∫–æ–º–º–µ–Ω—Ç—ã –≤ —Å–∫–æ–±–∫–∞—Ö –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
    UPPER = r'[[A-Za-z ]+[\:\]]' # —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–æ–≥–æ –∫—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç (BOBBY:)
    LETTERS = r'[^a-zA-Z\'.,!? ]' # –≤—Å–µ —á—Ç–æ –Ω–µ –±—É–∫–≤—ã –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª 
    SPACES = r'([ ])\1+' # –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –º–µ–Ω—è–µ–º –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
    DOTS = r'[\.]+' # –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –º–µ–Ω—è–µ–º –Ω–∞ —Ç–æ—á–∫—É
    SYMB = r"[^\w\d'\s]" # –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∫—Ä–æ–º–µ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∞

    def clean_subs(subs):
        subs = subs[1:] # —É–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∫–ª–∞–º–Ω—ã–π —Å—É–±—Ç–∏—Ç—Ä
        txt = re.sub(HTML, ' ', subs.text) # html —Ç—ç–≥–∏ –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
        txt = re.sub(COMMENTS, ' ', txt) # –∫–æ–º–º–µ–Ω—Ç—ã –≤ —Å–∫–æ–±–∫–∞—Ö –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
        txt = re.sub(UPPER, ' ', txt) # —É–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–æ–≥–æ –∫—Ç–æ –≥–æ–≤–æ—Ä–∏—Ç (BOBBY:)
        txt = re.sub(LETTERS, ' ', txt) # –≤—Å–µ —á—Ç–æ –Ω–µ –±—É–∫–≤—ã –º–µ–Ω—è–µ–º –Ω–∞ –ø—Ä–æ–±–µ–ª
        txt = re.sub(DOTS, r'.', txt) # –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ –º–µ–Ω—è–µ–º –Ω–∞ —Ç–æ—á–∫—É
        txt = re.sub(SPACES, r'\1', txt) # –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø—Ä–æ–±–µ–ª—ã –º–µ–Ω—è–µ–º –Ω–∞ –æ–¥–∏–Ω –ø—Ä–æ–±–µ–ª
        txt = re.sub(SYMB, '', txt) # –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –∫—Ä–æ–º–µ –∞–ø–æ—Å—Ç—Ä–æ—Ñ–∞ –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        txt = re.sub('www', '', txt) # –∫–æ–µ-–≥–¥–µ –æ—Å—Ç–∞—ë—Ç—Å—è www, —Ç–æ –∂–µ –º–µ–Ω—è–µ–º –Ω–∞ –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É
        txt = txt.lstrip() # –æ–±—Ä–µ–∑–∫–∞ –ø—Ä–æ–±–µ–ª–æ–≤ —Å–ª–µ–≤–∞
        txt = txt.encode('ascii', 'ignore').decode() # —É–¥–∞–ª—è–µ–º –≤—Å–µ —á—Ç–æ –Ω–µ ascii —Å–∏–º–≤–æ–ª—ã   
        txt = txt.lower() # —Ç–µ–∫—Å—Ç –≤ –Ω–∏–∂–Ω–∏–π —Ä–µ–≥–∏—Å—Ç—Ä
        return txt    

    #nlp = spacy.load("en_core_web_sm")
    nlp = en_core_web_sm.load()


    punctuations = string.punctuation

    stopwords = spacy.lang.en.stop_words.STOP_WORDS

    def spacy_tokenizer(sentence):
        tokens = nlp(sentence)

        tokens = [word.lemma_.lower().strip() if word.lemma_ != "PROPN" else word.lower_ for word in tokens]
    
        tokens = [word for word in tokens if word not in stopwords and word not in punctuations]

        return tokens


    #with open('C:/Users/Olga/Documents/Masterskay_2/model.pkl', 'rb') as dataset:
    #    model = pickle.load(dataset)
    with open(os.path.dirname(__file__) + '/model.pkl', 'rb') as dataset: 
        model = pickle.load(dataset)



    r = requests.get(path)
    with open("minemaster1.zip", "wb") as code:
        code.write(r.content)
    r_img = requests.get(path_img)
    with open("image.jpg", "wb") as code:
        code.write(r_img.content)    
    
    with zipfile.ZipFile("minemaster1.zip", 'r') as zip_ref:
        zip_ref.extractall()    
    
    sub = clean_subs(pysrt.open(zipfile.ZipFile("minemaster1.zip").namelist()[0], encoding='iso-8859-1'))

    #data = pd.DataFrame({'Subtitles': sub}, index=[0])
    data = pd.Series(sub)

    prediction = model.predict(data)
    st.sidebar.header('–†–µ–∑—É–ª—å—Ç–∞—Ç—ã',)
    st.sidebar.write(f'–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω –¥–ª—è: {name}' )
    #st.sidebar.write(f'–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: {(str(prediction).strip("[]"))}')
    st.sidebar.write(f"–£—Ä–æ–≤–µ–Ω—å —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ —è–∑—ã–∫–∞: {''.join(prediction)}")
    image = Image.open("image.jpg")
    st.sidebar.image(image)
    st.balloons()  
    
    os.remove(zipfile.ZipFile("minemaster1.zip").namelist()[0])
    os.remove(zipfile.ZipFile("minemaster1.zip").namelist()[1])
    os.remove("minemaster1.zip")
    os.remove( "image.jpg")
    
    #os.remove(os.path.dirname(__file__) + '/'+ (zipfile.ZipFile("minemaster1.zip").namelist()[0]))
    #os.remove(os.path.dirname(__file__) + '/'+ (zipfile.ZipFile("minemaster1.zip").namelist()[1]))
    #os.remove(os.path.dirname(__file__) + '/'+ "minemaster1.zip")
    #os.remove(os.path.dirname(__file__) + '/'+ "image.jpg")
                              


# In[ ]:




